{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import  PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from uuid import uuid4\n",
    "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import warnings\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "warnings.filterwarnings('ignore')\n",
    "import chromadb\n",
    "# Jupyter-specific imports\n",
    "from IPython.display import display\n",
    "# Set environment variable for protobuf\n",
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF loaded successfully: AI.pdf\n"
     ]
    }
   ],
   "source": [
    "# Load PDF\n",
    "local_path = \"AI.pdf\"\n",
    "if local_path:\n",
    "    loader = UnstructuredPDFLoader(file_path=local_path)\n",
    "    doc = loader.load()\n",
    "    print(f\"PDF loaded successfully: {local_path}\")\n",
    "else:\n",
    "    print(\"Upload a PDF file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text split into 22 chunks\n"
     ]
    }
   ],
   "source": [
    "# Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=300)\n",
    "chunks = text_splitter.split_documents(doc)\n",
    "print(f\"Text split into {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai_embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "persistent_client = chromadb.PersistentClient(path=\"db/chroma_store\")\n",
    "vector_store_from_client = Chroma(\n",
    "    client=persistent_client,\n",
    "    collection_name=\"collection_name\",\n",
    "    embedding_function=genai_embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: [Document(metadata={'source': 'AI.pdf'}, page_content='BỘ GIÁO DỤC VÀ ĐÀO TẠO TRƯỜNG ĐẠI HỌC KINH TẾ QUỐC DÂN\\n\\nTIỂU LUẬN Học phần: Học máy (Machine Learning) Đề tài: Nghiên cứu và cài đặt chương trình phần mềm sử dụng kỹ thuật DecisionTreeCART để dự đoán chất lượng của các loại rượu. Đánh giá hiệu quả của mô hình phân lớp\\n\\nNhóm sinh viên Lớp học phần Giảng viên hướng dẫn : TS. Lưu Minh Tuấn\\n\\n: Nhóm 4 : 01\\n\\nHà Nội, Năm 2024\\n\\nMỤC LỤC CHƯƠNG 1: GIỚI THIỆU ĐỀ TÀI ......................................................................... 4\\n\\n1.1. Tính cấp thiết của đề tài ................................................................................ 4\\n\\n1.2. Mục tiêu của đề tài ........................................................................................... 4\\n\\n1.3. Đối tượng và phạm vi nghiên cứu ................................................................... 4\\n\\n1.4. Phương pháp nghiên cứu................................................................................ 5\\n\\nCHƯƠNG 2: CÁC CƠ SỞ LÝ THUYẾT ................................................................ 6\\n\\n2.1. Giới thiệu về cây quyết định: .......................................................................... 6\\n\\n2.1.2. Thuật toán cây quyết định phân lớp (Decision Tree Classification Algorithm): .......................................................................................................... 7\\n\\n2.1.3. Ưu và nhược điểm của cây quyết định ..................................................... 8\\n\\n2.1.4. Thuật toán Cây quyết định hoạt động như thế nào? ............................. 10\\n\\n2.2. Thuật toán CART (Classification and Regression Trees). ........................... 10\\n\\n2.2.1. Ý tưởng .................................................................................................... 11\\n\\n2.2.2. Độ thuần nhất Gini (Gini impurity) ....................................................... 11\\n\\n2.2.3. Điều kiện dừng ......................................................................................... 12'), Document(metadata={'source': 'AI.pdf'}, page_content='2.2.2. Độ thuần nhất Gini (Gini impurity) ....................................................... 11\\n\\n2.2.3. Điều kiện dừng ......................................................................................... 12\\n\\n2.2.4. Pruning .................................................................................................... 13\\n\\nCHƯƠNG 3: CÀI ĐẶT THỬ NGHIỆM ................................................................ 15\\n\\n3.1. Bài toán: ......................................................................................................... 15\\n\\na. Dataset: .......................................................................................................... 15\\n\\nb. Ngôn ngữ và những thư viện cần thiết: ........................................................ 16\\n\\n3.2 . Phân tích dữ liệu khám phá ban đầu (EDA) và tiền xử lý dữ liệu: ............ 17\\n\\na. Feature Engineering: .................................................................................... 20\\n\\nb. Chọn đặc trưng: ............................................................................................ 21\\n\\n3.3. Chia dữ liệu:................................................................................................... 21\\n\\n3.4. Xây dựng mô hình cơ bản: ............................................................................ 23\\n\\n3.5. Kỹ thuật DecisionTree Pruning .................................................................... 24\\n\\n3.6. Điều chỉnh siêu tham số: FINE TUNNING VỚI RandomSearchCV .......... 24\\n\\n3.7. Model tối ưu ................................................................................................... 25\\n\\n3.8. Nhận xét và Đánh giá..................................................................................... 25\\n\\n3.9. Những features đặc trưng nhất ..................................................................... 26\\n\\n3.10. Màn hình chức năng sử dụng thư viện Streamlit & Joblib: ...................... 27\\n\\n1\\n\\nSTT\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6'), Document(metadata={'source': 'AI.pdf'}, page_content='3.9. Những features đặc trưng nhất ..................................................................... 26\\n\\n3.10. Màn hình chức năng sử dụng thư viện Streamlit & Joblib: ...................... 27\\n\\n1\\n\\nSTT\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\nDANH SÁCH THÀNH VIÊN NHÓM 4\\n\\nHọ và tên\\n\\nMã sinh viên\\n\\nNguyễn Ngọc Long\\n\\n11223918\\n\\nNguyễn Trần Trung\\n\\n11226625\\n\\nNguyễn Khánh Linh\\n\\n11223556\\n\\nVũ Thị Xuân Hương\\n\\n11222736\\n\\nNguyên Thị Khánh Huyền\\n\\n11222930\\n\\nTrần Thị Huyền Anh\\n\\n11220632\\n\\nVai trò\\n\\nNhóm trưởng\\n\\nThành viên\\n\\nThành viên\\n\\nThành viên\\n\\nThành viên\\n\\nThành viên\\n\\n2\\n\\nLỜI MỞ ĐẦU\\n\\nHọc máy là một lĩnh vực thuộc trí tuệ nhân tạo (AI), sử dụng các thuật toán được huấn luyện để tìm kiếm các mẫu và mối tương quan trong các tập dữ liệu lớn, từ đó đưa ra những quyết định và dự đoán tối ưu dựa trên quá trình phân tích. Trong thực tế, Học máy hiện đang được ứng dụng rộng rãi trong nhiều lĩnh vực như khai thác dữ liệu, chẩn đoán y tế, phát hiện gian lận thẻ tín dụng, phân tích thị trường chứng khoán, phân loại chuỗi DNA, nhận dạng giọng nói và chữ viết, dịch tự động, v.v. Tuy nhiên, để tránh sai lệch và đảm bảo tính chính xác, quá trình phân tích dữ liệu trong Học máy vẫn cần sự can thiệp và lựa chọn kỹ thuật từ con người.\\n\\nNhận thấy tiềm năng rộng lớn của Học máy, chúng em quyết định nghiên cứu về việc dự đoán chất lượng các loại rượu thông qua việc áp dụng các phương pháp học máy. Trong phạm vi bài tiểu luận này, chúng em tập trung nghiên cứu và triển khai chương trình phần mềm sử dụng kỹ thuật cây quyết định CART để dự đoán chất lượng rượu dựa trên các biến số liên quan.\\n\\nChúng em xin chân thành cảm ơn!\\n\\n3\\n\\nCHƯƠNG 1: GIỚI THIỆU ĐỀ TÀI\\n\\n1.1. Tính cấp thiết của đề tài'), Document(metadata={'source': 'AI.pdf'}, page_content='Chúng em xin chân thành cảm ơn!\\n\\n3\\n\\nCHƯƠNG 1: GIỚI THIỆU ĐỀ TÀI\\n\\n1.1. Tính cấp thiết của đề tài\\n\\nChất lượng của rượu vang đóng vai trò quan trọng đối với cả người tiêu dùng lẫn ngành công nghiệp rượu. Phương pháp truyền thống để đánh giá chất lượng rượu thông qua các chuyên gia thường tốn nhiều thời gian và công sức. Hiện nay, các mô hình học máy đã trở thành công cụ quan trọng, thay thế một phần công việc của con người. Tuy nhiên, không phải tất cả các đặc điểm của rượu đều liên quan đến việc dự đoán chất lượng tốt hơn, mà chỉ có một số tính năng chính là cần thiết.\\n\\nKhi đánh giá chất lượng của một chai rượu, mỗi người thường có quan điểm khác nhau. Đối với nhiều nhà phê bình rượu, chất lượng thường liên quan đến việc phân biệt rượu \"ngon\" hay \"dở\", dựa trên sở thích cá nhân và sự hài lòng hoặc không hài lòng về rượu. Đánh giá này thường được đặt trong bối cảnh các tiêu chuẩn đã được thiết lập cho từng loại rượu, do vậy chất lượng không chỉ mang tính chủ quan mà còn phụ thuộc vào cả yếu tố cảm quan bên trong (mùi vị) và yếu tố bên ngoài (ngữ cảnh).\\n\\nTuy nhiên, con người chúng ta chủ yếu bị ảnh hưởng bởi thị giác, do đó, hình dáng chai rượu thường ảnh hưởng đến cách chúng ta cảm nhận hương thơm và vị giác của rượu. Vì thế, để tránh thiên vị, những người thực sự muốn đánh giá rượu một cách khách quan thường nếm thử rượu mà không biết về nguồn gốc của nó, sử dụng các ly nếm rượu tiêu chuẩn hóa màu đen (ISO), chỉ đánh giá chất lượng dựa trên những gì rượu truyền tải đến khứu giác và vị giác.\\n\\nVậy làm thế nào để đánh giá được một lượng lớn rượu về chất lượng tốt hay xấu? Bằng cách áp dụng học máy, chúng ta có thể sử dụng phương pháp sàng lọc, hay còn gọi là kỹ thuật phân lớp, để so sánh các mẫu rượu chất lượng cao với những mẫu khác, từ đó tìm ra các mẫu có chất lượng tương đồng nhất.\\n\\n1.2. Mục tiêu của đề tài'), Document(metadata={'source': 'AI.pdf'}, page_content='1.2. Mục tiêu của đề tài\\n\\nSử dụng kỹ thuật cây quyết định CART để phân tích và dự đoán chất lượng của các loại rượu dựa trên bộ dữ liệu winequality-red.csv, sử dụng các biến số có sẵn trong tập dữ liệu để dự đoán nhãn chất lượng.\\n\\nThực hiện phân lớp dựa trên mô hình CART và sử dụng tập test để kiểm tra độ chính xác, từ đó đánh giá hiệu quả của mô hình dựa trên các chỉ số như độ chính xác, độ nhạy và các tiêu chí khác trong quá trình dự đoán.\\n\\n1.3. Đối tượng và phạm vi nghiên cứu\\n\\nĐối tượng: kỹ thuật cây quyết định CART, bài toán tối ưu hóa tổ hợp. - Phạm vi nghiên cứu: dự trên bộ dữ liệu winequality-red.csv\\n\\n4\\n\\n1.4. Phương pháp nghiên cứu\\n\\nTrong tiểu luận này, nhóm sử dụng phương pháp nghiên cứu thực nghiệm để thu\\n\\nthập bằng chứng có thể kiểm chứng để đi đến kết quả nghiên cứu.\\n\\n5\\n\\nCHƯƠNG 2: CÁC CƠ SỞ LÝ THUYẾT\\n\\n2.1. Giới thiệu về cây quyết định:\\n\\nTrong lĩnh vực học máy, cây quyết định là một mô hình dự báo, nghĩa là nó ánh\\n\\nxạ từ các quan sát về một sự vật hoặc hiện tượng đến các kết luận về giá trị mục tiêu của\\n\\nsự vật, hiện tượng đó. Mỗi nút bên trong của cây tương ứng với một biến, và các nhánh\\n\\nnối giữa các nút biểu thị giá trị cụ thể của biến đó. Mỗi nút lá đại diện cho giá trị dự\\n\\nđoán của biến mục tiêu, dựa trên các giá trị của các biến được xác định từ đường đi từ\\n\\nnút gốc đến nút lá. Phương pháp học máy được sử dụng trong cây quyết định gọi là học\\n\\nbằng cây quyết định.\\n\\nCây quyết định là một công cụ mạnh mẽ trong việc phân loại và dự đoán. Sự hấp\\n\\ndẫn của phương pháp này nằm ở khả năng xử lý nhiều loại sự kiện, và khác với mạng\\n\\nnơ-ron, cây quyết định thể hiện các quy tắc, những quy tắc này dễ hiểu đối với con\\n\\nngười. Cây quyết định có nhiều ứng dụng, ví dụ như trong hệ thống thư tín của công ty,\\n\\nmột mô hình có thể dự đoán chính xác thành viên nào trong nhóm sẽ phản hồi một yêu\\n\\ncầu mà không cần biết chi tiết cách thức hoạt động của mô hình. Trong một số trường\\n\\nhợp khác, cây quyết định có thể giải thích lý do của một quyết định, điều này rất quan'), Document(metadata={'source': 'AI.pdf'}, page_content='một mô hình có thể dự đoán chính xác thành viên nào trong nhóm sẽ phản hồi một yêu\\n\\ncầu mà không cần biết chi tiết cách thức hoạt động của mô hình. Trong một số trường\\n\\nhợp khác, cây quyết định có thể giải thích lý do của một quyết định, điều này rất quan\\n\\ntrọng trong các ứng dụng yêu cầu sự phân loại hay dự đoán chính xác.\\n\\n2.1.1. Ví dụ về cây quyết định:\\n\\nThuật toán cây quyết định trong hình ảnh cung cấp một ví dụ rõ ràng về cách\\n\\nphân loại một loài động vật thành hai nhóm: \"Chim\" hoặc \"Thú\", dựa trên một loạt câu\\n\\nhỏi về đặc điểm sinh học.\\n\\n6\\n\\nCâu hỏi đầu tiên tại nút gốc là \"Động vật có thể bay không?\" Đây là điểm phân\\n\\nnhánh đầu tiên. Nếu câu trả lời là \"có\", thuật toán tiếp tục hỏi \"Động vật này có đẻ trứng\\n\\nkhông?\" Nếu đúng, động vật đó được phân loại là Chim. Nếu không, nó sẽ được phân\\n\\nloại là Thú (ví dụ như dơi). Ngược lại, nếu động vật không thể bay, thuật toán sẽ chuyển\\n\\nsang câu hỏi tiếp theo: \"Động vật này có lông vũ không?\" Nếu đúng, động vật đó vẫn là\\n\\nChim (ví dụ như chim cánh cụt, không bay nhưng có lông vũ). Nếu không, động vật đó\\n\\nsẽ được phân loại là Thú.\\n\\nThuật toán cây quyết định này sử dụng một chuỗi các câu hỏi phân cấp để phân\\n\\nloại đối tượng dựa trên các đặc điểm rõ ràng và cụ thể. Điều này cho thấy cây quyết định\\n\\nlà một công cụ trực quan và hiệu quả trong việc phân loại các đối tượng trong nhiều bài\\n\\ntoán học máy và dữ liệu thực tế, nhờ vào việc chia nhỏ dữ liệu thành các quyết định dựa\\n\\ntrên các thuộc tính riêng lẻ.\\n\\n2.1.2. Thuật toán cây quyết định phân lớp (Decision Tree Classification\\n\\nAlgorithm):\\n\\nThuật toán cây quyết định là một kỹ thuật học có giám sát, được sử dụng rộng rãi\\n\\ntrong các bài toán phân loại và hồi quy. Nó thuộc nhóm các mô hình dự đoán có khả\\n\\nnăng học hỏi từ dữ liệu đã có nhãn để đưa ra dự đoán cho các dữ liệu mới. Thuật toán\\n\\ncây quyết định đặc biệt hữu ích khi cần giải thích kết quả vì nó chia nhỏ các vấn đề phức\\n\\ntạp thành các quyết định nhỏ hơn và rõ ràng. Điều này giúp cho cây quyết định trở thành'), Document(metadata={'source': 'AI.pdf'}, page_content='năng học hỏi từ dữ liệu đã có nhãn để đưa ra dự đoán cho các dữ liệu mới. Thuật toán\\n\\ncây quyết định đặc biệt hữu ích khi cần giải thích kết quả vì nó chia nhỏ các vấn đề phức\\n\\ntạp thành các quyết định nhỏ hơn và rõ ràng. Điều này giúp cho cây quyết định trở thành\\n\\nmột trong những thuật toán dễ hiểu và dễ triển khai trong thực tế.\\n\\nCấu trúc của cây quyết định bao gồm ba thành phần chính: nút gốc, các nút bên\\n\\ntrong (nút quyết định), và các nút lá. Nút gốc là nơi bắt đầu quá trình phân loại hoặc dự\\n\\nđoán, thường là dựa trên thuộc tính quan trọng nhất trong tập dữ liệu. Các nút bên trong\\n\\nđại diện cho các câu hỏi hoặc các phép kiểm tra liên quan đến các thuộc tính của dữ liệu,\\n\\nvà mỗi nhánh từ một nút bên trong tương ứng với một giá trị hoặc một kết quả của phép\\n\\nkiểm tra đó. Cuối cùng, các nút lá đại diện cho các kết quả cuối cùng, tức là phân loại\\n\\nhoặc dự đoán của thuật toán.\\n\\nCách hoạt động của thuật toán cây quyết định bắt đầu từ nút gốc, tại đây thuật\\n\\ntoán chọn thuộc tính tốt nhất để phân chia dữ liệu dựa trên một số tiêu chí như Gini\\n\\nimpurity, entropy (trong thuật toán ID3), hoặc information gain (lợi ích thông tin). Dữ\\n\\n7\\n\\nliệu sẽ được phân chia thành các tập con dựa trên giá trị của thuộc tính đó, và quá trình\\n\\nnày tiếp tục đệ quy tại mỗi nút con cho đến khi tất cả các tập con đều thuần nhất hoặc\\n\\nđạt đến điều kiện dừng.\\n\\nThuật toán này được gọi là cây quyết định bởi vì quá trình phân chia dữ liệu diễn\\n\\nra tương tự như cách các nhánh cây phân nhánh từ một thân cây chính. Các quyết định\\n\\nđược đưa ra liên tiếp tại các nút nội bộ, và kết quả cuối cùng xuất hiện ở các nút lá,\\n\\ntương tự như cách các nhánh cây kết thúc bằng các lá. Sự đơn giản trong cấu trúc và khả\\n\\nnăng chia nhỏ các vấn đề phức tạp thành các quyết định tuần tự là yếu tố làm nên sức\\n\\nmạnh của cây quyết định.\\n\\nSơ đồ dưới đây giải thích cấu trúc chung của cây quyết định:\\n\\n2.1.3. Ưu và nhược điểm của cây quyết định\\n\\nƯu điểm của cây quyết định:'), Document(metadata={'source': 'AI.pdf'}, page_content='năng chia nhỏ các vấn đề phức tạp thành các quyết định tuần tự là yếu tố làm nên sức\\n\\nmạnh của cây quyết định.\\n\\nSơ đồ dưới đây giải thích cấu trúc chung của cây quyết định:\\n\\n2.1.3. Ưu và nhược điểm của cây quyết định\\n\\nƯu điểm của cây quyết định:\\n\\nClear Visualization: Thuật toán đơn giản để hiểu, diễn giải và trực quan vì ý\\n\\ntưởng chủ yếu được sử dụng trong cuộc sống hàng ngày của chúng ta. Đầu ra của\\n\\ncây quyết định có thể được con người giải thích một cách dễ dàng.\\n\\nĐơn giản và dễ hiểu: Cây quyết định trông giống như các câu lệnh if-else đơn\\n\\ngiản và rất dễ hiểu.\\n\\nCây quyết định có thể được sử dụng cho cả bài toán phân loại và bài toán hồi\\n\\nquy.\\n\\nCây quyết định có thể xử lý cả biến liên tục và biến phân loại.\\n\\nKhông yêu cầu feature scaling: Không yêu cầu feature scaling (standardization\\n\\nvà normalization) trong trường hợp cây quyết định vì nó sử dụng phương pháp\\n\\ntiếp cận dựa trên quy tắc thay vì tính toán.\\n\\n8\\n\\nXử lý các tham số phi tuyến tính một cách hiệu quả: Các tham số phi tuyến tính\\n\\nkhông ảnh hưởng đến hiệu suất của cây quyết định không giống như các thuật\\n\\ntoán dựa trên đường cong. Vì vậy, nếu có sự không tuyến tính cao giữa các biến\\n\\nđộc lập, cây quyết định có thể hoạt động tốt hơn so với các thuật toán dựa trên\\n\\nđường cong khác.\\n\\nCây quyết định có thể tự động xử lý các giá trị bị thiếu.\\n\\nCây quyết định thường mạnh mẽ đối với các trường hợp ngoại lệ và có thể xử lý\\n\\nchúng một cách tự động.\\n\\nThời gian đào tạo ít hơn: Thời gian đào tạo ít hơn so với rừng ngẫu nhiên\\n\\n(Random Forest ) vì nó chỉ tạo ra một cây không giống như rừng của các cây\\n\\ntrong Random Forest.\\n\\nNhược điểm của Cây quyết định:\\n\\nOverfitting: Đây là vấn đề chính của cây quyết định. Nó thường dẫn đến việc quá\\n\\nkhớp dữ liệu mà cuối cùng dẫn đến dự đoán sai. Để fit với dữ liệu (ngay cả dữ\\n\\nliệu nhiễu), nó tiếp tục tạo ra các nút mới và cuối cùng cây trở nên quá phức tạp\\n\\nđể diễn giải, dẫn đến mất khả năng tổng quát hóa. Nó hoạt động rất tốt trên dữ'), Document(metadata={'source': 'AI.pdf'}, page_content='khớp dữ liệu mà cuối cùng dẫn đến dự đoán sai. Để fit với dữ liệu (ngay cả dữ\\n\\nliệu nhiễu), nó tiếp tục tạo ra các nút mới và cuối cùng cây trở nên quá phức tạp\\n\\nđể diễn giải, dẫn đến mất khả năng tổng quát hóa. Nó hoạt động rất tốt trên dữ\\n\\nliệu được đào tạo nhưng bắt đầu mắc nhiều lỗi trên dữ liệu không nhìn thấy được.\\n\\nPhương sai cao: Như đã đề cập ở điểm 1, cây quyết định thường dẫn đến việc quá\\n\\nkhớp dữ liệu. Chính vì thế, có rất nhiều khả năng sai lệch cao trong đầu ra, dẫn\\n\\nđến nhiều sai sót trong ước tính cuối cùng và cho thấy kết quả không chính xác\\n\\ncao. Quá khớp dẫn đến phương sai cao.\\n\\nKhông ổn định: Việc thêm một điểm dữ liệu mới có thể dẫn đến việc tạo lại cây\\n\\ntổng thể và tất cả các nút cần được tính toán lại và tạo lại.\\n\\nBị ảnh hưởng bởi nhiễu: Một chút dữ liệu nhiễu có thể làm cho nó không ổn định,\\n\\ndẫn đến dự đoán sai.\\n\\nKhông phù hợp với tập dữ liệu lớn: Nếu kích thước dữ liệu lớn, thì một cây đơn\\n\\nlẻ có thể phát triển phức tạp và dẫn đến quá khớp. Vì vậy, trong trường hợp này,\\n\\nchúng ta nên sử dụng Random Forest thay vì một cây quyết định. Để khắc phục\\n\\nnhững hạn chế của cây quyết định, chúng ta nên sử dụng Random Forest để không\\n\\ndựa vào một cây nào. Nó tạo ra một rừng cây và đưa ra quyết định dựa trên số\\n\\nphiếu bầu.\\n\\n9\\n\\n2.1.4. Thuật toán Cây quyết định hoạt động như thế nào?\\n\\nTrong cây quyết định, để dự đoán lớp của tập dữ liệu đã cho, thuật toán bắt đầu\\n\\ntừ nút gốc của cây. Thuật toán này so sánh các giá trị của thuộc tính gốc với thuộc tính\\n\\nbản ghi (tập dữ liệu thực) và dựa trên sự so sánh, đi theo nhánh và nhảy đến nút tiếp\\n\\ntheo.\\n\\nĐối với nút tiếp theo, thuật toán lại so sánh giá trị thuộc tính với các nút con khác\\n\\nvà di chuyển xa hơn. Nó tiếp tục quá trình cho đến khi nó đạt đến nút lá của cây. Quy\\n\\ntrình hoàn chỉnh có thể được hiểu rõ hơn bằng cách sử dụng thuật toán dưới đây:\\n\\nBước 1: Bắt đầu xây dựng cây quyết định với nút gốc (S), nơi chứa toàn bộ tập'), Document(metadata={'source': 'AI.pdf'}, page_content='và di chuyển xa hơn. Nó tiếp tục quá trình cho đến khi nó đạt đến nút lá của cây. Quy\\n\\ntrình hoàn chỉnh có thể được hiểu rõ hơn bằng cách sử dụng thuật toán dưới đây:\\n\\nBước 1: Bắt đầu xây dựng cây quyết định với nút gốc (S), nơi chứa toàn bộ tập\\n\\ndữ liệu ban đầu. Nút gốc này là điểm khởi đầu để phân chia dữ liệu dựa trên các thuộc\\n\\ntính.\\n\\nBước 2: Tìm thuộc tính tốt nhất trong tập dữ liệu bằng cách sử dụng các phép đo\\n\\nlựa chọn thuộc tính (Attribute Selection Measure - ASM), chẳng hạn như entropy, Gini\\n\\nimpurity, hoặc information gain. Đây là bước quan trọng để xác định thuộc tính có khả\\n\\nnăng phân loại dữ liệu tốt nhất.\\n\\nBước 3: Dựa trên thuộc tính tốt nhất được chọn, chia tập dữ liệu S thành các tập\\n\\ncon nhỏ hơn. Mỗi tập con sẽ chứa các phần tử tương ứng với các giá trị khác nhau của\\n\\nthuộc tính đó.\\n\\nBước 4: Tạo một nút quyết định mới đại diện cho thuộc tính tốt nhất vừa chọn.\\n\\nNút này sẽ có các nhánh dẫn tới các tập con đã được chia nhỏ ở bước trước.\\n\\nBước 5: Đệ quy lặp lại quá trình này đối với các tập con vừa tạo ra. Thuật toán\\n\\ntiếp tục chọn thuộc tính tốt nhất cho mỗi tập con và phân chia tiếp tục cho đến khi không\\n\\nthể chia nhỏ thêm nữa, tại đó các nút cuối cùng sẽ trở thành nút lá, đại diện cho phân\\n\\nloại cuối cùng hoặc kết quả dự đoán.\\n\\n2.2. Thuật toán CART (Classification and Regression Trees).\\n\\nThuật toán CART (cây phân loại và hồi quy - Classification and Regression\\n\\nTree), là một thuật toán được Leo Breiman giới thiệu năm 1984, dùng để xây dựng cây\\n\\nquyết định đa năng, với mục đích phân loại và hồi quy dữ liệu.\\n\\n10\\n\\n2.2.1. Ý tưởng\\n\\nÝ tưởng chính của thuật toán CART là xây dựng một cây quyết định bằng cách\\n\\nphân chia dữ liệu thành các nhóm con dựa trên thuộc tính tốt nhất tại mỗi nút. Thuật\\n\\ntoán này sử dụng các tiêu chí đánh giá như Gini impurity (cho phân loại) và mean\\n\\nsquared error (MSE) (cho hồi quy) để đảm bảo rằng các nhóm con được phân chia có\\n\\ntính thuần nhất cao, từ đó đưa ra các quyết định hoặc dự đoán chính xác hơn.'), Document(metadata={'source': 'AI.pdf'}, page_content='toán này sử dụng các tiêu chí đánh giá như Gini impurity (cho phân loại) và mean\\n\\nsquared error (MSE) (cho hồi quy) để đảm bảo rằng các nhóm con được phân chia có\\n\\ntính thuần nhất cao, từ đó đưa ra các quyết định hoặc dự đoán chính xác hơn.\\n\\nSo với các thuật toán cây quyết định khác như ID3 và C4.5, CART có một số\\n\\nkhác biệt rõ rệt. Thứ nhất, CART sử dụng Gini impurity và MSE làm chỉ số đo lường\\n\\nđể chọn thuộc tính tốt nhất, trong khi ID3 và C4.5 sử dụng entropy và information gain\\n\\nđể đánh giá thuộc tính. Ngoài ra, CART luôn thực hiện phân chia nhị phân, ngay cả khi\\n\\nthuộc tính có nhiều giá trị khác nhau, trong khi ID3 và C4.5 cho phép phân chia thành\\n\\nnhiều nhánh dựa trên tất cả các giá trị của thuộc tính đó.\\n\\nCuối cùng, CART có cơ chế cắt tỉa cây (pruning) sau khi cây đã được xây dựng\\n\\nđể tránh overfitting, bằng cách loại bỏ các nhánh không cần thiết. C4.5 cũng có tính\\n\\nnăng cắt tỉa cây, nhưng ID3 không thực hiện việc này một cách rõ ràng, điều này có thể\\n\\ndẫn đến cây phân loại phức tạp hơn và dễ bị overfitting.\\n\\n2.2.2. Độ thuần nhất Gini (Gini impurity)\\n\\nĐộ thuần nhất Gini (Gini Impurity) là một chỉ số dùng để đo lường mức độ \"thuần\\n\\nnhất\" của một tập dữ liệu, hay nói cách khác, nó đánh giá mức độ hỗn loạn hoặc không\\n\\nđồng nhất của các nhãn trong tập dữ liệu tại một nút của cây quyết định. Giá trị của độ\\n\\nthuần nhất Gini dao động từ 0 đến 1:\\n\\nGini = 0: Tập dữ liệu hoàn toàn thuần nhất, tức là tất cả các phần tử trong tập\\n\\nđều thuộc cùng một lớp.\\n\\nGini = 1: Tập dữ liệu hoàn toàn hỗn loạn, tức là các phần tử được phân phối\\n\\nđồng đều giữa các lớp khác nhau.\\n\\nCông thức tính Gini Impurity cho một tập dữ liệu chứa nhiều lớp như sau:\\n\\n1\\n\\n1\\n\\ni\\n\\n2. Trong đó:\\n\\n𝐺𝐺𝐺𝐺𝐺𝐺𝐺𝐺 =\\n\\n𝑛𝑛 𝑖𝑖= − ∑ i là tỷ lệ của các phần tử thuộc lớp ●\\n\\n𝑝𝑝\\n\\ntrong tập dữ liệu.\\n\\n𝑝𝑝\\n\\n𝐺𝐺\\n\\n11\\n\\nn là số lớp trong tập dữ liệu.\\n\\nCách cây quyết định sử dụng Gini Impurity để phân chia nhánh:\\n\\n1. Tính Gini Impurity tại nút hiện tại: Ban đầu, thuật toán sẽ tính độ thuần nhất'), Document(metadata={'source': 'AI.pdf'}, page_content='1\\n\\n1\\n\\ni\\n\\n2. Trong đó:\\n\\n𝐺𝐺𝐺𝐺𝐺𝐺𝐺𝐺 =\\n\\n𝑛𝑛 𝑖𝑖= − ∑ i là tỷ lệ của các phần tử thuộc lớp ●\\n\\n𝑝𝑝\\n\\ntrong tập dữ liệu.\\n\\n𝑝𝑝\\n\\n𝐺𝐺\\n\\n11\\n\\nn là số lớp trong tập dữ liệu.\\n\\nCách cây quyết định sử dụng Gini Impurity để phân chia nhánh:\\n\\n1. Tính Gini Impurity tại nút hiện tại: Ban đầu, thuật toán sẽ tính độ thuần nhất\\n\\nGini cho tập dữ liệu tại nút gốc, xác định mức độ hỗn loạn của dữ liệu trước khi\\n\\nphân chia.\\n\\n2. Phân chia dữ liệu dựa trên thuộc tính: Cây quyết định thử phân chia dữ liệu dựa\\n\\ntrên các thuộc tính khác nhau. Với mỗi thuộc tính, thuật toán chia tập dữ liệu\\n\\nthành hai tập con dựa trên giá trị của thuộc tính đó (cây CART luôn chia thành\\n\\nhai nhánh nhị phân). Sau đó, độ thuần nhất Gini của các tập con này sẽ được\\n\\ntính toán.\\n\\n3. Tính Gini Impurity trung bình có trọng số: Độ thuần nhất Gini của mỗi tập con\\n\\nsau khi phân chia sẽ được tính toán và trung bình có trọng số của Gini Impurity\\n\\ncủa các tập con cũng được xác định. Trọng số được tính dựa trên kích thước của\\n\\nmỗi tập con so với tổng dữ liệu ban đầu.\\n\\n4. Chọn thuộc tính có Gini Impurity thấp nhất: Thuộc tính nào làm giảm Gini\\n\\nImpurity nhiều nhất (tức là có sự cải thiện tốt nhất về mức độ thuần nhất sau khi\\n\\nphân chia) sẽ được chọn để chia nhánh tại nút đó. Mục tiêu là chọn thuộc tính\\n\\nlàm cho các tập con sau khi chia có mức độ thuần nhất cao nhất (tức là Gini\\n\\nImpurity thấp nhất).\\n\\n5. Lặp lại quá trình: Sau khi đã chọn thuộc tính và phân chia dữ liệu, cây quyết\\n\\nđịnh tiếp tục áp dụng quá trình này một cách đệ quy cho các tập con cho đến\\n\\nkhi đạt điều kiện dừng (tập dữ liệu hoàn toàn thuần nhất hoặc không còn thuộc\\n\\ntính nào để chia).\\n\\n2.2.3. Điều kiện dừng\\n\\nTrong các thuật toán cây quyết định nói chung và CART nói riêng, nếu ta tiếp\\n\\ntục phân chia các nút chưa thuần nhất, kết quả là một cây quyết định mà mọi điểm\\n\\ntrong tập huấn luyện đều được dự đoán chính xác (giả sử không có trường hợp hai\\n\\ninput giống nhau nhưng cho output khác nhau). Tuy nhiên, cây sẽ trở nên rất phức tạp'), Document(metadata={'source': 'AI.pdf'}, page_content='tục phân chia các nút chưa thuần nhất, kết quả là một cây quyết định mà mọi điểm\\n\\ntrong tập huấn luyện đều được dự đoán chính xác (giả sử không có trường hợp hai\\n\\ninput giống nhau nhưng cho output khác nhau). Tuy nhiên, cây sẽ trở nên rất phức tạp\\n\\nvới nhiều nút lá và chỉ chứa một vài điểm dữ liệu tại mỗi nút. Điều này làm tăng nguy\\n\\ncơ xảy ra overfitting.\\n\\n12\\n\\nĐể tránh tình trạng overfitting, một số phương pháp có thể được áp dụng.\\n\\nNgưỡng tối thiểu số lượng mẫu: Dừng phân chia khi số lượng mẫu trong một\\n\\ntập con nhỏ hơn một giá trị ngưỡng nhất định, giúp tránh việc tạo ra các nhánh\\n\\nquá nhỏ và không có ý nghĩa.\\n\\nĐộ thuần nhất cao: Nếu tất cả các mẫu trong một tập con đều thuộc cùng một\\n\\nlớp (phân loại) hoặc có giá trị rất tương đồng (hồi quy), thuật toán sẽ dừng phân\\n\\nchia.\\n\\nKhông còn thuộc tính để phân chia: Khi tất cả các thuộc tính đã được sử dụng\\n\\nhết, thuật toán không thể tiếp tục phân chia thêm và dừng tại đó.\\n\\nCải thiện lỗi không đáng kể: Dừng phân chia nếu việc tiếp tục không làm giảm\\n\\nđáng kể các chỉ số như Gini impurity (phân loại) hoặc mean squared error (hồi\\n\\nquy).\\n\\nGiới hạn độ sâu của cây: Cây sẽ dừng phát triển khi đạt đến độ sâu tối đa do\\n\\nngười dùng đặt trước, giúp kiểm soát độ phức tạp của cây.\\n\\n2.2.4. Pruning\\n\\nCắt tỉa (pruning) trong thuật toán CART (Classification and Regression Trees) là\\n\\nmột kỹ thuật quan trọng giúp cải thiện hiệu quả của cây quyết định và tránh hiện tượng\\n\\nquá khớp (overfitting). Khi xây dựng cây quyết định, nếu cây quá phức tạp và có quá\\n\\nnhiều nút hoặc nhánh, mô hình có thể khớp quá sát với dữ liệu huấn luyện, làm giảm khả\\n\\nnăng tổng quát hóa của nó đối với dữ liệu mới. Cắt tỉa giúp giảm kích thước của cây, giữ\\n\\nlại những phần quan trọng nhất và loại bỏ những nhánh ít hữu ích.\\n\\n\\n\\nHai loại cắt tỉa trong thuật toán CART:\\n\\nCắt tỉa trước (Pre-pruning): Quá trình cắt tỉa này diễn ra trong khi cây đang\\n\\nđược xây dựng. Thuật toán sẽ ngừng phân chia thêm tại một nút khi đạt đến'), Document(metadata={'source': 'AI.pdf'}, page_content='lại những phần quan trọng nhất và loại bỏ những nhánh ít hữu ích.\\n\\n\\n\\nHai loại cắt tỉa trong thuật toán CART:\\n\\nCắt tỉa trước (Pre-pruning): Quá trình cắt tỉa này diễn ra trong khi cây đang\\n\\nđược xây dựng. Thuật toán sẽ ngừng phân chia thêm tại một nút khi đạt đến\\n\\nmột số điều kiện nhất định, chẳng hạn như khi số lượng mẫu trong một\\n\\nnhánh nhỏ hơn một ngưỡng cụ thể, hoặc khi mức độ cải thiện của việc phân\\n\\nchia là không đáng kể. Điều này giúp hạn chế sự phát triển của cây từ sớm,\\n\\nngăn chặn việc tạo ra quá nhiều nhánh không cần thiết.\\n\\nCắt tỉa sau (Post-pruning): Đây là quá trình cắt tỉa sau khi cây đã được xây\\n\\ndựng hoàn chỉnh. Thuật toán sẽ loại bỏ các nhánh không mang lại nhiều\\n\\nthông tin, tức là những nhánh mà việc phân chia không cải thiện đáng kể độ\\n\\n13\\n\\nchính xác của mô hình. Mục tiêu là làm đơn giản hóa cấu trúc cây mà không\\n\\nlàm giảm quá nhiều hiệu suất.\\n\\nTiêu chí cắt tỉa\\n\\nGiá trị lỗi của cây (tree error): Thuật toán sẽ tính toán lỗi của cây với và\\n\\nkhông có nhánh đó. Nếu việc loại bỏ một nhánh giúp giảm lỗi tổng thể\\n\\nhoặc không làm tăng lỗi đáng kể, nhánh đó sẽ bị loại bỏ.\\n\\nLỗi cắt tỉa với cây quyết định CART: Trong cắt tỉa sau, thuật toán có thể\\n\\nsử dụng tiêu chí lỗi cắt tỉa (pruning error), chẳng hạn như dựa trên dữ\\n\\nliệu xác thực (validation set) hoặc phương pháp Cross-validation, để\\n\\nđánh giá hiệu quả của mỗi nhánh sau khi cắt tỉa.\\n\\nQuá trình cắt tỉa trong CART\\n\\nTrong thuật toán CART, cắt tỉa thường được thực hiện bằng cách sử dụng\\n\\nphương pháp chi phí phức tạp cắt tỉa (Cost Complexity Pruning), một kỹ thuật cắt tỉa\\n\\nsau (post-pruning). Quá trình này kết hợp giữa chi phí lỗi và độ phức tạp của cây để\\n\\nloại bỏ các nhánh không cần thiết. Công thức để tính toán chi phí phức tạp là:\\n\\nTrong đó:\\n\\n𝑅𝑅𝑅𝑅(𝑇𝑇) = 𝑅𝑅(𝑇𝑇) + 𝑅𝑅 ∗ |𝑇𝑇|\\n\\n\\n\\nlà lỗi của cây T.\\n\\n\\n\\n\\n\\nlà tham số điều chỉnh giữa độ phức tạp và lỗi.\\n\\n𝑅𝑅(𝑇𝑇) T 𝑅𝑅\\n\\nlà số lư\\n\\nng nút trong cây.\\n\\n∣ Giá trị α được sử dụng để điều chỉnh mức độ mà độ phức tạp của cây được ưu\\n\\n∣\\n\\nợ'), Document(metadata={'source': 'AI.pdf'}, page_content=\"Trong đó:\\n\\n𝑅𝑅𝑅𝑅(𝑇𝑇) = 𝑅𝑅(𝑇𝑇) + 𝑅𝑅 ∗ |𝑇𝑇|\\n\\n\\n\\nlà lỗi của cây T.\\n\\n\\n\\n\\n\\nlà tham số điều chỉnh giữa độ phức tạp và lỗi.\\n\\n𝑅𝑅(𝑇𝑇) T 𝑅𝑅\\n\\nlà số lư\\n\\nng nút trong cây.\\n\\n∣ Giá trị α được sử dụng để điều chỉnh mức độ mà độ phức tạp của cây được ưu\\n\\n∣\\n\\nợ\\n\\ntiên so với độ chính xác. Khi α tăng lên, cây sẽ ưu tiên đơn giản hóa hơn và nhiều\\n\\nnhánh có thể bị cắt bỏ.\\n\\n14\\n\\nCHƯƠNG 3: CÀI ĐẶT THỬ NGHIỆM\\n\\n3.1. Bài toán:\\n\\nNghiên cứu và cài đặt chương trình phần mềm sử dụng kỹ thuật cây quyết định CART để dự đoán chất lượng (quality) của các loại rượu theo các biến còn lại. Từ kết quả phân lớp và dữ liệu trong tập test, đánh giá hiệu quả của mô hình dự đoán.\\n\\nĐầu vào (Input): Tệp dữ liệu của các loại chất hoá học có trong một loại rượu. - Đầu ra (Output): Dự đoán chất lượng của loại rượu nêu trên.\\n\\na. Dataset:\\n\\nCác thuộc tính có trong dataset:\\n\\n1. fixed acidity: Hầu hết các axit liên quan đến rượu vang là hoặc cố định hoặc\\n\\nkhông bay hơi (không dễ bay hơi).\\n\\n2. volatile acidity: Lượng axit axetic trong rượu vang ở mức quá cao có thể\\n\\nkhiến rượu có vị chua của giấm, rất khó chịu.\\n\\n3. citric acid: Với số lượng không đáng kể, axit citric có thể thêm 'độ tươi' và\\n\\nhương vị cho rượu vang.\\n\\n4. residual sugar: Lượng đường còn lại sau khi ngừng lên men, hiếm có loại\\n\\nrượu nào có chỉ số dưới 1 gam / lít.\\n\\n5. chlorides: Lượng muối trong rượu.\\n\\n6. free sulfur dioxide: Dạng tự do của SO2 tồn tại ở trạng thái cân bằng giữa\\n\\nphân tử SO2 (ở dạng khí hòa tan) và ion bisulfit\\n\\n7. total sulfur dioxide: Lượng các dạng liên kết và tự do của S02, ở nồng độ\\n\\nthấp, hầu như không thể phát hiện được SO2 trong rượu vang, nhưng ta có thể\\n\\nphát hiện SO2 ở thể tự do.\\n\\n8. density: Tỷ trọng của nước gần với tỷ trọng của hỗn hợp nước phụ thuộc vào\\n\\nphần trăm độ cồn và lượng đường.\\n\\n15\\n\\n9. pH: Mô tả mức độ axit hoặc bazơ của một loại rượu trên thang điểm từ 0 (rất\\n\\naxit) đến 14 (rất kiềm), hầu hết các loại rượu đều nằm trong khoảng từ 3-4.\\n\\n10. sulfates: Một chất phụ gia rượu vang có thể góp phần làm tăng nồng độ khí\"), Document(metadata={'source': 'AI.pdf'}, page_content=\"phần trăm độ cồn và lượng đường.\\n\\n15\\n\\n9. pH: Mô tả mức độ axit hoặc bazơ của một loại rượu trên thang điểm từ 0 (rất\\n\\naxit) đến 14 (rất kiềm), hầu hết các loại rượu đều nằm trong khoảng từ 3-4.\\n\\n10. sulfates: Một chất phụ gia rượu vang có thể góp phần làm tăng nồng độ khí\\n\\nsulfur dioxide (S02), hoạt động như một chất kháng khuẩn.\\n\\n11. alcohol: Nồng độ cồn của rượu.\\n\\n12. quality: Kết quả đầu ra là chất lượng của rượu (dựa trên dữ liệu cảm quan,\\n\\ncho điểm từ 0 đến 10).\\n\\nb. Ngôn ngữ và những thư viện cần thiết:\\n\\nNgôn ngữ:\\n\\nPython cho phép các lập trình viên thực hiện viết code ngắn, dễ đọc hơn trong khi các thuật toán phức tạp và quy trình làm việc linh hoạt của Machine Learning, AI\\n\\nThư viện:\\n\\nNhững thư viện được sử dụng để giải quyết bài toán này bao gồm:\\n\\nnumpy: thư viện phục vụ việc tính toán - pandas: thư viện cung cấp các cấu trúc dữ liệu seaborn: thư viện mô hình hoá dữ liệu - - matplotlib: thư viện mô hình hoá dữ liệu -\\n\\nscikit-learn (sklearn): thư viện cung cấp một tập các công cụ xử lý các bài toán machine learning và statistical modeling gồm: classification, regression, clustering, và dimensionality reduction… imbalanced-learn (imblearn): thư viện chuyên xử lý các bộ dữ liệu mất cân bằng, nơi mà số lượng mẫu của các lớp không đồng đều.\\n\\n\\n\\n16\\n\\n3.2 . Phân tích dữ liệu khám phá ban đầu (EDA) và tiền xử lý dữ liệu:\\n\\nTừ biểu đồ phân bố giá trị của các thuộc tính ta có thể thấy :\\n\\nCác thuộc tính 'fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\\n\\n'chlorides', 'free sulfur dioxide', 'total sulfur dioxide' và 'density' có phân bố dữ liệu không đều, có nhiều giá trị nằm ở phía bên trái của biểu đồ.\\n\\n→ Điều này có thể làm cho mô hình dễ bị nhiễu bởi các giá trị ngoại lai.\\n\\nFixed acidity:\\n\\nPhân phối lệch phải, với đỉnh chính ở khoảng 6-8. Rượu chất lượng cao (màu\\n\\nđỏ) có xu hướng có độ axit cố định cao hơn.\\n\\nVolatile acidity:\\n\\n17\\n\\nPhân phối khá cân đối, với đỉnh ở khoảng 0.3-0.4. Rượu chất lượng thấp có xu\\n\\nhướng có độ axit bay hơi cao hơn.\"), Document(metadata={'source': 'AI.pdf'}, page_content='Fixed acidity:\\n\\nPhân phối lệch phải, với đỉnh chính ở khoảng 6-8. Rượu chất lượng cao (màu\\n\\nđỏ) có xu hướng có độ axit cố định cao hơn.\\n\\nVolatile acidity:\\n\\n17\\n\\nPhân phối khá cân đối, với đỉnh ở khoảng 0.3-0.4. Rượu chất lượng thấp có xu\\n\\nhướng có độ axit bay hơi cao hơn.\\n\\nCitric acid:\\n\\nPhân phối đa phương thức với nhiều đỉnh. Rượu chất lượng cao có xu hướng có\\n\\nhàm lượng axit citric cao hơn.\\n\\nResidual sugar:\\n\\nPhân phối lệch phải mạnh, với đa số mẫu có lượng đường dư thấp (dưới 5).\\n\\nKhông có sự khác biệt rõ ràng giữa các mức chất lượng.\\n\\nChlorides:\\n\\nPhân phối lệch phải, tập trung chủ yếu ở khoảng 0.05-0.1. Rượu chất lượng cao\\n\\ncó xu hướng có hàm lượng clorua thấp hơn.\\n\\nFree sulfur dioxide và Total sulfur dioxide:\\n\\nCả hai đều có phân phối lệch phải.\\n\\nRượu chất lượng cao có xu hướng có hàm lượng sulfur dioxide tự do và tổng số\\n\\nthấp hơn.\\n\\nDensity:\\n\\nPhân phối gần như chuẩn, tập trung quanh 0.995-1.000. Không có sự khác biệt\\n\\nrõ ràng giữa các mức chất lượng.\\n\\npH:\\n\\nPhân phối gần như chuẩn, tập trung quanh 3.0-3.4. Không có sự khác biệt rõ\\n\\nràng giữa các mức chất lượng.\\n\\nSulphates:\\n\\nPhân phối lệch phải, với đỉnh ở khoảng 0.5. Rượu chất lượng cao có xu hướng\\n\\ncó hàm lượng sunphat cao hơn.\\n\\nAlcohol:\\n\\nPhân phối lệch phải nhẹ. Rượu chất lượng cao có xu hướng có nồng độ cồn cao\\n\\nhơn.\\n\\nQuality:\\n\\nPhân phối gần như chuẩn, với đa số mẫu tập trung ở mức chất lượng trung bình\\n\\n(5-6). Ít mẫu ở mức chất lượng rất thấp hoặc rất cao.\\n\\nTính không cân đối:\\n\\nPhân phối hơi lệch phải, với đuôi dài hơn về phía chất lượng cao.\\n\\n18\\n\\nKết luận:\\n\\nNhiều đặc trưng có mối tương quan với chất lượng rượu, đặc biệt là volatile\\n\\nacidity, citric acid, chlorides, sulfur dioxide, sulphates và alcohol.\\n\\nPhần lớn các đặc trưng có phân phối lệch, cho thấy sự không đồng đều trong dữ liệu. → Ta tiến hành chia bins, áp dụng SMOTE và tạo ra các features mới để máy học tốt hơn\\n\\n→ Bộ dữ liệu khá đồng nhất với 1599 mẫu cho mỗi đặc trưng, không có giá trị null.'), Document(metadata={'source': 'AI.pdf'}, page_content=\"Phần lớn các đặc trưng có phân phối lệch, cho thấy sự không đồng đều trong dữ liệu. → Ta tiến hành chia bins, áp dụng SMOTE và tạo ra các features mới để máy học tốt hơn\\n\\n→ Bộ dữ liệu khá đồng nhất với 1599 mẫu cho mỗi đặc trưng, không có giá trị null.\\n\\n→ Sự mất cân đối trong phân phối có thể ảnh hưởng đến việc xây dựng mô hình dự đoán.\\n\\nTương quan của các biến ảnh hưởng tới chất lượng rượu:\\n\\n19\\n\\nNhiều đặc trưng có mối tương quan với chất lượng rượu, đặc biệt là volatile\\n\\nacidity, citric acid, chlorides, sulfur dioxide, sulphates và alcohol.\\n\\n→ Các biến liên quan có tương quan khá vừa (correlation < 0.8) vậy ta nên chọn hết để build model.\\n\\nSử dụng IQR method kiểm tra Outliers\\n\\n→ Các c\\n\\nt có outliers: ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\\n\\n'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH ', 'sulphates',\\n\\nộ 'alcohol', 'quality']\\n\\nKết luận: Ta sẽ giữ outliers do dataset này khá bé đồng nhất và không có giá trị null.\\n\\na. Feature Engineering:\\n\\nCần biến đổi những feature có tương quan với nhau thấp hoặc cao mà có thể gây ra đa cộng tuyến, có liên hệ với nhau theo biến, có phân phối giá trị không đồng đều, đó được coi là “những features nhiễu” nên để giúp máy học tốt hơn mà không bị ảnh hưởng bởi những features nhiễu này, ta tiến hành việc nhóm, chia bins.\\n\\n20\\n\\nb. Chọn đặc trưng:\\n\\nSau khi thêm biến mới, ta kiểm tra lại correlation và sử dụng heatmap hiển thị mức độ tương quan giữa các biến, giúp dễ dàng nhận biết các biến có tương quan mạnh với nhau.\\n\\nSử dụng hàm correlation ( đưa vào 1 function) giúp tự động xác định các cặp biến có hệ số tương quan tuyệt đối lớn hơn 0.7. Điều này rất hữu ích cho việc lựa chọn đặc trưng và tránh đa cộng tuyến trong mô hình học máy. - Sử dụng heatmap hiển thị trực quan ma trận tương quan giữa các biến\\n\\n3.3. Chia dữ liệu:\\n\\nĐể xử lý bài toán này, ta tiến hành tách dữ liệu thành 2 phần: 1 dataframe (X)\"), Document(metadata={'source': 'AI.pdf'}, page_content='3.3. Chia dữ liệu:\\n\\nĐể xử lý bài toán này, ta tiến hành tách dữ liệu thành 2 phần: 1 dataframe (X)\\n\\nmới gồm tất cả các cột ngoại trừ quality, là tập hợp các đặc trưng và 1 biến mục tiêu y được gán từ cột quality. Ta tiến hành tính toán tần suất của từng giá trị trong cột quality.\\n\\n21\\n\\n→ Ta thấy tập y có sự mất cân bằng khá lớn giữa các giá trị.\\n\\nĐể có thể cân bằng lại sự lệch của tập mẫu, giảm tình trạng bias, ta sử dụng kỹ\\n\\nthuật SMOTE. Cụ thể hơn, ở đây chúng em sử dụng SMOTETomek (kết hợp kỹ thuật SMOTE và kỹ thuật Tomek Links) thay vì SMOTE thông thường để loại bỏ các mẫu dữ liệu của lớp đa số nằm gần biên giới giữa các lớp, giúp cải thiện hiệu quả phân loại.\\n\\n→ Sau khi sử dụng kết hợp 2 kỹ thuật trên, ta thấy tập y đã có sự cân bằng giữa các giá\\n\\ntrị.\\n\\nBên cạnh đó, một trong những vấn đề phổ biến của bài toán phân lớp là sự mất cân bằng trong dữ liệu tập mẫu dẫn đến vấn đề bias và overfitting. Vì vậy, để đảm bảo tính nguyên vẹn của dataset cùng với đó là giảm thiểu sự ảnh hưởng của các giá trị ngoại lai mà không loại bỏ chúng, ta sử dụng RobustScaler - bộ công cụ chuẩn hóa dữ liệu.\\n\\nNgoài ra, chúng em còn tiến hành chia dataset thành 2 tập dữ liệu là tập train và tập test với tỉ lệ là 80-20. Mục đích của việc này là để chuẩn bị dữ liệu cho học máy 2 tập riêng biệt và áp dụng co giãn đặc trưng để làm cho các đặc trưng phù hợp với mô hình.\\n\\n22\\n\\n3.4. Xây dựng mô hình cơ bản:\\n\\nĐịnh nghĩa, khởi tạo và huấn luyện model dt - Decision Tree Classifier dựa trên bộ\\n\\ndữ liệu training, với tham số random state là 42.\\n\\nSau khi sử dụng model huấn luyện để dự đoán trên tập test, nhóm đánh giá hiệu\\n\\nnăng mô hình dựa trên độ chính xác và ma trận nhầm lẫn.\\n\\nKết quả:\\n\\nAccuracy: 0.9029 (~90.29%) Hiệu suất theo từng lớp:\\n\\nLớp 3.0: Hiệu suất rất tốt với precision 0.95, recall 0.96 và F1-score 0.95. Mô hình có khả năng nhận diện và phân loại lớp này rất tốt.'), Document(metadata={'source': 'AI.pdf'}, page_content='năng mô hình dựa trên độ chính xác và ma trận nhầm lẫn.\\n\\nKết quả:\\n\\nAccuracy: 0.9029 (~90.29%) Hiệu suất theo từng lớp:\\n\\nLớp 3.0: Hiệu suất rất tốt với precision 0.95, recall 0.96 và F1-score 0.95. Mô hình có khả năng nhận diện và phân loại lớp này rất tốt.\\n\\nLớp 4.0: Hiệu suất khá tốt với precision 0.96, recall 0.94 và F1-score 0.95. ● Lớp 5.0: Hiệu suất trung bình với precision 0.78, recall 0.76 và F1-score 0.77. Có thể cần cải thiện, đặc biệt là recall.\\n\\nLớp 6.0: Hiệu suất khá cân bằng trong các lớp, với precision 0.81, recall 0.83 và F1-score 0.82.\\n\\nLớp 7.0: Hiệu suất khá tốt với precision 0.98, recall 0.94 và F1-score 0.96. ● Lớp 8.0: Hiệu suất rất tốt, gần như ngang bằng với lớp 3.0, có precision 0.94, recall 0.98 và F1-score 0.96.\\n\\n=> TÓM LẠI:\\n\\nMô hình hoạt động khá tốt, với độ chính xác tổng thể là 90%. - Mô hình hoạt động tốt nhất cho nhãn 3-4-7-8, với Precision và Recall cao (~95% và 95%).\\n\\nMức độ dự đoán đúng của mô hình cho nhãn 6 và nhãn 5 cũng khá cao, nhưng có\\n\\nmột số trường hợp dự đoán nhầm lẫn giữa nhãn 6 và nhãn 5 (dự đoán nhãn 6 thành nhãn 5 và ngược lại).\\n\\n23\\n\\n=> Ta cần ưu tiên hơn vể sự cân bằng giữa precision và recall để cho lấy được kết quả sát nhất so với tập mẫu.\\n\\n3.5. Kỹ thuật DecisionTree Pruning\\n\\nTạo một model mới dt_test sử dụng DecisionTreeClassifier. Sau đó cho chạy vòng\\n\\nlặp for để tìm ra ccp_alpha tối ưu nhất cho mô hình cây quyết định. Kết quả:\\n\\nChạy lại đánh giá hiệu năng bằng độ chính xác và ma trận nhầm lẫn, đạt được kết quả mới như sau:\\n\\nSau khi áp dụng pruning, có một số cải thiện nhỏ trong hiệu suất của mô hình:\\n\\nĐộ chính xác tổng thể tăng nhẹ (lên 0.2%). - Lớp 5.0 và 6.0 có sự cải thiện rõ rệt nhất. - Lớp 5.0: Trước: 0.78 / 0.76 / 0.77 - Sau: 0.78 / 0.78 / 0.78\\n\\n=> Cải thiện nhẹ ở cả ba chỉ số.\\n\\nLớp 6.0: Trước: 0.81 / 0.83 / 0.82 - Sau: 0.82 / 0.82 / 0.82\\n\\n=> Cải thiện precision và F1-score, recall bị giảm nhẹ nhung mô hình đã cho thấy sự cân bằng rất tốt ở lớp này .\\n\\nCòn lại các lớp khác không đổi\\n\\nK\\n\\nt lu\\n\\nn:\\n\\n⇒'), Document(metadata={'source': 'AI.pdf'}, page_content=\"=> Cải thiện nhẹ ở cả ba chỉ số.\\n\\nLớp 6.0: Trước: 0.81 / 0.83 / 0.82 - Sau: 0.82 / 0.82 / 0.82\\n\\n=> Cải thiện precision và F1-score, recall bị giảm nhẹ nhung mô hình đã cho thấy sự cân bằng rất tốt ở lớp này .\\n\\nCòn lại các lớp khác không đổi\\n\\nK\\n\\nt lu\\n\\nn:\\n\\n⇒\\n\\nPruning đã giúp mô hình cải thiện một chút về hiệu suất tổng thể, đặc biệt là ế ậ\\n\\nĐộ chính xác tổng thể chỉ tăng nhẹ, nhưng pruning giúp mô hình trở nên tổng quát\\n\\nhơn và tránh overfitting.\\n\\n3.6. Điều chỉnh siêu tham số: FINE TUNNING VỚI RandomSearchCV\\n\\nSử dụng Fine tunning với các tham số như sau:\\n\\n24\\n\\nTham số max_depth giá trị từ 2 đến 20 - Tham số min_sample_leafs giá trị từ 1 đến 15 - Tham số min_sample_split giá trị từ 1 đến 15 - Tham số criterion là “gini” hoặc “entropy” - Tham số splitter là “best” hoặc “random”\\n\\nSử dụng hàm RandomSearchCV với ưu tiên scoring là độ chính xác (accuracy) cho\\n\\nmô hình đã prunning, để tìm ra mô hình tối ưu.\\n\\nKết quả:\\n\\nVậy model tối ưu của ta sẽ có những tham số, criterion='entropy', max_depth=15,\\n\\nmin_samples_leaf=1,random_state=42. Tuy nhiên, để có được sự cân bằng giữa precision ⇒ và recall , ta nên dùng criterion 'gini' (phù hợp hơn cho CART và dẫn đến sự phân bổ lớp học cân bằng hơn).\\n\\n3.7. Model tối ưu\\n\\nSử dụng mô hình tối ưu với các tham số vừa tìm được và chạy lại độ chính xác và\\n\\nma trận nhầm lẫn, ta được kết quả mới:\\n\\n3.8. Nhận xét và Đánh giá\\n\\nAccuracy: Sau khi pruning: 0.9042 Sau khi RandomizedSearchCV: 0.9055 Precision, Recall, F1-score:\\n\\nClass 3.0, 4.0, 7.0, 8.0: Không có thay đổi đáng kể giữa các mô hình. Các chỉ số\\n\\nprecision, recall và f1-score gần như giữ nguyên ở cả hai mô hình.\\n\\nClass 5.0 và 6.0:\\n\\nClass 5.0: F1-score sau khi RandomizedSearchCV tăng từ 0.78 lên 0.79.\\n\\nPrecision và recall cũng cải thiện nhẹ.\\n\\n25\\n\\nClass 6.0: F1-score tăng từ 0.82 lên 0.83. Precision và recall cũng tăng nhẹ.\\n\\nConfusion Matrix: Cả hai confusion matrix rất giống nhau, ngoại trừ một vài thay đổi nhỏ:\\n\\nClass 5.0: Sau khi RandomizedSearchCV, số mẫu dự đoán đúng (93) tăng lên một\"), Document(metadata={'source': 'AI.pdf'}, page_content='Precision và recall cũng cải thiện nhẹ.\\n\\n25\\n\\nClass 6.0: F1-score tăng từ 0.82 lên 0.83. Precision và recall cũng tăng nhẹ.\\n\\nConfusion Matrix: Cả hai confusion matrix rất giống nhau, ngoại trừ một vài thay đổi nhỏ:\\n\\nClass 5.0: Sau khi RandomizedSearchCV, số mẫu dự đoán đúng (93) tăng lên một\\n\\nđơn vị so với mô hình sau khi pruning (92).\\n\\nClass 6.0: Số mẫu dự đoán đúng (118) cũng tăng lên 1 so với sau pruning (117). - Macro và Weighted Averages: Cả hai mô hình đều có macro và weighted averages gần như giống nhau, với một\\n\\nsự cải thiện nhỏ trong precision và f1-score sau khi áp dụng RandomizedSearchCV.\\n\\n⇒\\n\\nKết Luận : - Model tối ưu sau RandomSearchCV và pruning có độ chính xác (90.55 %) và hiệu suất cao hơn. Đặc biệt, Precision, Recall, và F1-Score của tất cả các nhãn đều được cải thiện hoặc duy trì tốt hơn.\\n\\nSố mẫu dự đoán đúng cho các nhãn cũng tăng lên, đặc biệt là nhãn 0 và nhãn 2,\\n\\ngiúp mô hình dự đoán chính xác hơn cho các nhãn này.\\n\\nTA XÂY DỰNG ĐƯỢC 1 MÔ HÌNH TỐI ƯU NHẤT SỬ DỤNG\\n\\nDecisionTreeClassifier ⇒\\n\\n3.9. Những features đặc trưng nhất\\n\\nDùng hàm feature_importances để lấy ra các features đặc trưng nhất và trực quan\\n\\nhóa, ta được kết quả:\\n\\n26\\n\\n3.10. Màn hình chức năng sử dụng thư viện Streamlit & Joblib:\\n\\nTÀI LIỆU THAM KHẢO\\n\\n27\\n\\nTÀI LIỆU THAM KHẢO\\n\\n1. Tek4. (n.d.). Thuật toán CART. Tek4. Truy cập ngày [1/10/2024], từ\\n\\nhttps://tek4.vn/khoa-hoc/machine-learning-co-ban/thuat-toan-cart.\\n\\n2. Selawsky, J. (2021, December 6). Decision trees with CART algorithm. Medium.\\n\\nTruy cập ngày [2/10/2024], từ https://medium.com/geekculture/decision-trees-\\n\\nwith-cart-algorithm-7e179acee8ff.\\n\\n3. GeeksforGeeks. (n.d.). CART (Classification and Regression Tree) in Machine\\n\\nLearning. Truy cập ngày [2/10/2024], từ https://www.geeksforgeeks.org/cart-\\n\\nclassification-and-regression-tree-in-machine-learning/.\\n\\n28')]\n",
      "UUIDs: ['675a0bde-e2b6-46ce-b644-09ba50abec84', '00f6dd34-2088-4e47-af77-41f94fcc79ec', '4c844326-5f89-40de-ad7f-cc455baef54e', '7e0d03c0-37e3-4964-99f8-0617868e827e', '3a91d73b-058c-4638-a8db-eccded6e9862', '0c0c403e-d4ad-457d-9380-016bcb3d37d3', 'd6cadd15-1ec4-4af5-ab22-f44ff0860645', '812edad2-9f72-4c50-bdf7-e434203e130a', '695666c6-b774-4a6f-b1d9-2d597af79ba6', '8c317555-6f61-4a02-94ed-e2d9846f9378', '569df2be-0b00-4c8e-beb1-82e03fe30836', '8ebdb31c-0b75-45d8-a89f-3989493d1fea', 'db3a35aa-13ed-4f15-9db4-d35825acf1d0', '0df69e04-a853-44d9-8719-57a15a2426b0', '002b6339-8505-4164-9164-bbe2a9119f36', 'ff1e1899-c93b-4df9-b1c8-2249927aa72d', 'b1533119-e84d-43ab-ba8e-30b11f43e669', 'fdfd020d-82ef-4a98-b474-362d6e036445', '2bc1f8bd-851f-4244-bb48-9be51764e2b8', '9356de98-2028-4172-b27d-94c164499417', '180005b7-bdf7-4495-b24e-13d747b8bd6e', '29446e68-d15c-4453-8f1d-9aaaddc08a6a']\n",
      "Lengths match: True\n"
     ]
    }
   ],
   "source": [
    "documents = [Document(page_content=chunk.page_content, metadata=chunk.metadata) for chunk in chunks]\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "print(\"Documents:\", documents)\n",
    "print(\"UUIDs:\", uuids)\n",
    "print(\"Lengths match:\", len(documents) == len(uuids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents added successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vector_store_from_client.add_documents(documents = documents, ids=uuids)\n",
    "    print(\"Documents added successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "genllm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_template = \"\"\"\n",
    "        You are an advanced AI assistant whose name is \"Nexus\" tasked with answering user {question} based on the provided document's extracted {context}\n",
    "        You can also analyse data from CSV/Excel files reports, database.\n",
    "        Use the following rules:\n",
    "        - Always utilize all relevant context to generate a complete and accurate response.\n",
    "        - If the context lacks information, clearly state: \"Answer not found in the provided context.\"\n",
    "        - Respond in the same language as the input question.\n",
    "        - Provide structured responses (e.g., lists or bullet points) when applicable for clarity.\n",
    "        - Prioritize the most relevant information and avoid irrelevant details.\n",
    "        Context:\n",
    "        {context}\n",
    "        Question:\n",
    "        {question}\n",
    "        Response:.\n",
    "        \"\"\"\n",
    "human_message = HumanMessagePromptTemplate.from_template(\n",
    "    template=prompt_template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [human_message]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up retriever\n",
    "retriever = vector_store_from_client.as_retriever(search_type=\"similarity\",  search_kwargs={\"k\": 5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = load_qa_chain(genllm, chain_type=\"stuff\", prompt=qa_prompt, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_qa = RetrievalQA(combine_documents_chain=qa_chain, retriever=retriever, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_pdf(question):\n",
    "    \"\"\"\n",
    "    Chat with the PDF using the RAG chain.\n",
    "    \"\"\"\n",
    "    return retriever_qa.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: \n",
      "        You are an advanced AI assistant whose name is \"Nexus\" tasked with answering user Cam onon based on the provided document's extracted Chúng em xin chân thành cảm ơn!\n",
      "\n",
      "3\n",
      "\n",
      "CHƯƠNG 1: GIỚI THIỆU ĐỀ TÀI\n",
      "\n",
      "1.1. Tính cấp thiết của đề tài\n",
      "\n",
      "Chất lượng của rượu vang đóng vai trò quan trọng đối với cả người tiêu dùng lẫn ngành công nghiệp rượu. Phương pháp truyền thống để đánh giá chất lượng rượu thông qua các chuyên gia thường tốn nhiều thời gian và công sức. Hiện nay, các mô hình học máy đã trở thành công cụ quan trọng, thay thế một phần công việc của con người. Tuy nhiên, không phải tất cả các đặc điểm của rượu đều liên quan đến việc dự đoán chất lượng tốt hơn, mà chỉ có một số tính năng chính là cần thiết.\n",
      "\n",
      "Khi đánh giá chất lượng của một chai rượu, mỗi người thường có quan điểm khác nhau. Đối với nhiều nhà phê bình rượu, chất lượng thường liên quan đến việc phân biệt rượu \"ngon\" hay \"dở\", dựa trên sở thích cá nhân và sự hài lòng hoặc không hài lòng về rượu. Đánh giá này thường được đặt trong bối cảnh các tiêu chuẩn đã được thiết lập cho từng loại rượu, do vậy chất lượng không chỉ mang tính chủ quan mà còn phụ thuộc vào cả yếu tố cảm quan bên trong (mùi vị) và yếu tố bên ngoài (ngữ cảnh).\n",
      "\n",
      "Tuy nhiên, con người chúng ta chủ yếu bị ảnh hưởng bởi thị giác, do đó, hình dáng chai rượu thường ảnh hưởng đến cách chúng ta cảm nhận hương thơm và vị giác của rượu. Vì thế, để tránh thiên vị, những người thực sự muốn đánh giá rượu một cách khách quan thường nếm thử rượu mà không biết về nguồn gốc của nó, sử dụng các ly nếm rượu tiêu chuẩn hóa màu đen (ISO), chỉ đánh giá chất lượng dựa trên những gì rượu truyền tải đến khứu giác và vị giác.\n",
      "\n",
      "Vậy làm thế nào để đánh giá được một lượng lớn rượu về chất lượng tốt hay xấu? Bằng cách áp dụng học máy, chúng ta có thể sử dụng phương pháp sàng lọc, hay còn gọi là kỹ thuật phân lớp, để so sánh các mẫu rượu chất lượng cao với những mẫu khác, từ đó tìm ra các mẫu có chất lượng tương đồng nhất.\n",
      "\n",
      "1.2. Mục tiêu của đề tài\n",
      "\n",
      "Chúng em xin chân thành cảm ơn!\n",
      "\n",
      "3\n",
      "\n",
      "CHƯƠNG 1: GIỚI THIỆU ĐỀ TÀI\n",
      "\n",
      "1.1. Tính cấp thiết của đề tài\n",
      "\n",
      "Chất lượng của rượu vang đóng vai trò quan trọng đối với cả người tiêu dùng lẫn ngành công nghiệp rượu. Phương pháp truyền thống để đánh giá chất lượng rượu thông qua các chuyên gia thường tốn nhiều thời gian và công sức. Hiện nay, các mô hình học máy đã trở thành công cụ quan trọng, thay thế một phần công việc của con người. Tuy nhiên, không phải tất cả các đặc điểm của rượu đều liên quan đến việc dự đoán chất lượng tốt hơn, mà chỉ có một số tính năng chính là cần thiết.\n",
      "\n",
      "Khi đánh giá chất lượng của một chai rượu, mỗi người thường có quan điểm khác nhau. Đối với nhiều nhà phê bình rượu, chất lượng thường liên quan đến việc phân biệt rượu \"ngon\" hay \"dở\", dựa trên sở thích cá nhân và sự hài lòng hoặc không hài lòng về rượu. Đánh giá này thường được đặt trong bối cảnh các tiêu chuẩn đã được thiết lập cho từng loại rượu, do vậy chất lượng không chỉ mang tính chủ quan mà còn phụ thuộc vào cả yếu tố cảm quan bên trong (mùi vị) và yếu tố bên ngoài (ngữ cảnh).\n",
      "\n",
      "Tuy nhiên, con người chúng ta chủ yếu bị ảnh hưởng bởi thị giác, do đó, hình dáng chai rượu thường ảnh hưởng đến cách chúng ta cảm nhận hương thơm và vị giác của rượu. Vì thế, để tránh thiên vị, những người thực sự muốn đánh giá rượu một cách khách quan thường nếm thử rượu mà không biết về nguồn gốc của nó, sử dụng các ly nếm rượu tiêu chuẩn hóa màu đen (ISO), chỉ đánh giá chất lượng dựa trên những gì rượu truyền tải đến khứu giác và vị giác.\n",
      "\n",
      "Vậy làm thế nào để đánh giá được một lượng lớn rượu về chất lượng tốt hay xấu? Bằng cách áp dụng học máy, chúng ta có thể sử dụng phương pháp sàng lọc, hay còn gọi là kỹ thuật phân lớp, để so sánh các mẫu rượu chất lượng cao với những mẫu khác, từ đó tìm ra các mẫu có chất lượng tương đồng nhất.\n",
      "\n",
      "1.2. Mục tiêu của đề tài\n",
      "\n",
      "Chúng em xin chân thành cảm ơn!\n",
      "\n",
      "3\n",
      "\n",
      "CHƯƠNG 1: GIỚI THIỆU ĐỀ TÀI\n",
      "\n",
      "1.1. Tính cấp thiết của đề tài\n",
      "\n",
      "Chất lượng của rượu vang đóng vai trò quan trọng đối với cả người tiêu dùng lẫn ngành công nghiệp rượu. Phương pháp truyền thống để đánh giá chất lượng rượu thông qua các chuyên gia thường tốn nhiều thời gian và công sức. Hiện nay, các mô hình học máy đã trở thành công cụ quan trọng, thay thế một phần công việc của con người. Tuy nhiên, không phải tất cả các đặc điểm của rượu đều liên quan đến việc dự đoán chất lượng tốt hơn, mà chỉ có một số tính năng chính là cần thiết.\n",
      "\n",
      "Khi đánh giá chất lượng của một chai rượu, mỗi người thường có quan điểm khác nhau. Đối với nhiều nhà phê bình rượu, chất lượng thường liên quan đến việc phân biệt rượu \"ngon\" hay \"dở\", dựa trên sở thích cá nhân và sự hài lòng hoặc không hài lòng về rượu. Đánh giá này thường được đặt trong bối cảnh các tiêu chuẩn đã được thiết lập cho từng loại rượu, do vậy chất lượng không chỉ mang tính chủ quan mà còn phụ thuộc vào cả yếu tố cảm quan bên trong (mùi vị) và yếu tố bên ngoài (ngữ cảnh).\n",
      "\n",
      "Tuy nhiên, con người chúng ta chủ yếu bị ảnh hưởng bởi thị giác, do đó, hình dáng chai rượu thường ảnh hưởng đến cách chúng ta cảm nhận hương thơm và vị giác của rượu. Vì thế, để tránh thiên vị, những người thực sự muốn đánh giá rượu một cách khách quan thường nếm thử rượu mà không biết về nguồn gốc của nó, sử dụng các ly nếm rượu tiêu chuẩn hóa màu đen (ISO), chỉ đánh giá chất lượng dựa trên những gì rượu truyền tải đến khứu giác và vị giác.\n",
      "\n",
      "Vậy làm thế nào để đánh giá được một lượng lớn rượu về chất lượng tốt hay xấu? Bằng cách áp dụng học máy, chúng ta có thể sử dụng phương pháp sàng lọc, hay còn gọi là kỹ thuật phân lớp, để so sánh các mẫu rượu chất lượng cao với những mẫu khác, từ đó tìm ra các mẫu có chất lượng tương đồng nhất.\n",
      "\n",
      "1.2. Mục tiêu của đề tài\n",
      "\n",
      "một mô hình có thể dự đoán chính xác thành viên nào trong nhóm sẽ phản hồi một yêu\n",
      "\n",
      "cầu mà không cần biết chi tiết cách thức hoạt động của mô hình. Trong một số trường\n",
      "\n",
      "hợp khác, cây quyết định có thể giải thích lý do của một quyết định, điều này rất quan\n",
      "\n",
      "trọng trong các ứng dụng yêu cầu sự phân loại hay dự đoán chính xác.\n",
      "\n",
      "2.1.1. Ví dụ về cây quyết định:\n",
      "\n",
      "Thuật toán cây quyết định trong hình ảnh cung cấp một ví dụ rõ ràng về cách\n",
      "\n",
      "phân loại một loài động vật thành hai nhóm: \"Chim\" hoặc \"Thú\", dựa trên một loạt câu\n",
      "\n",
      "hỏi về đặc điểm sinh học.\n",
      "\n",
      "6\n",
      "\n",
      "Câu hỏi đầu tiên tại nút gốc là \"Động vật có thể bay không?\" Đây là điểm phân\n",
      "\n",
      "nhánh đầu tiên. Nếu câu trả lời là \"có\", thuật toán tiếp tục hỏi \"Động vật này có đẻ trứng\n",
      "\n",
      "không?\" Nếu đúng, động vật đó được phân loại là Chim. Nếu không, nó sẽ được phân\n",
      "\n",
      "loại là Thú (ví dụ như dơi). Ngược lại, nếu động vật không thể bay, thuật toán sẽ chuyển\n",
      "\n",
      "sang câu hỏi tiếp theo: \"Động vật này có lông vũ không?\" Nếu đúng, động vật đó vẫn là\n",
      "\n",
      "Chim (ví dụ như chim cánh cụt, không bay nhưng có lông vũ). Nếu không, động vật đó\n",
      "\n",
      "sẽ được phân loại là Thú.\n",
      "\n",
      "Thuật toán cây quyết định này sử dụng một chuỗi các câu hỏi phân cấp để phân\n",
      "\n",
      "loại đối tượng dựa trên các đặc điểm rõ ràng và cụ thể. Điều này cho thấy cây quyết định\n",
      "\n",
      "là một công cụ trực quan và hiệu quả trong việc phân loại các đối tượng trong nhiều bài\n",
      "\n",
      "toán học máy và dữ liệu thực tế, nhờ vào việc chia nhỏ dữ liệu thành các quyết định dựa\n",
      "\n",
      "trên các thuộc tính riêng lẻ.\n",
      "\n",
      "2.1.2. Thuật toán cây quyết định phân lớp (Decision Tree Classification\n",
      "\n",
      "Algorithm):\n",
      "\n",
      "Thuật toán cây quyết định là một kỹ thuật học có giám sát, được sử dụng rộng rãi\n",
      "\n",
      "trong các bài toán phân loại và hồi quy. Nó thuộc nhóm các mô hình dự đoán có khả\n",
      "\n",
      "năng học hỏi từ dữ liệu đã có nhãn để đưa ra dự đoán cho các dữ liệu mới. Thuật toán\n",
      "\n",
      "cây quyết định đặc biệt hữu ích khi cần giải thích kết quả vì nó chia nhỏ các vấn đề phức\n",
      "\n",
      "tạp thành các quyết định nhỏ hơn và rõ ràng. Điều này giúp cho cây quyết định trở thành\n",
      "\n",
      "một mô hình có thể dự đoán chính xác thành viên nào trong nhóm sẽ phản hồi một yêu\n",
      "\n",
      "cầu mà không cần biết chi tiết cách thức hoạt động của mô hình. Trong một số trường\n",
      "\n",
      "hợp khác, cây quyết định có thể giải thích lý do của một quyết định, điều này rất quan\n",
      "\n",
      "trọng trong các ứng dụng yêu cầu sự phân loại hay dự đoán chính xác.\n",
      "\n",
      "2.1.1. Ví dụ về cây quyết định:\n",
      "\n",
      "Thuật toán cây quyết định trong hình ảnh cung cấp một ví dụ rõ ràng về cách\n",
      "\n",
      "phân loại một loài động vật thành hai nhóm: \"Chim\" hoặc \"Thú\", dựa trên một loạt câu\n",
      "\n",
      "hỏi về đặc điểm sinh học.\n",
      "\n",
      "6\n",
      "\n",
      "Câu hỏi đầu tiên tại nút gốc là \"Động vật có thể bay không?\" Đây là điểm phân\n",
      "\n",
      "nhánh đầu tiên. Nếu câu trả lời là \"có\", thuật toán tiếp tục hỏi \"Động vật này có đẻ trứng\n",
      "\n",
      "không?\" Nếu đúng, động vật đó được phân loại là Chim. Nếu không, nó sẽ được phân\n",
      "\n",
      "loại là Thú (ví dụ như dơi). Ngược lại, nếu động vật không thể bay, thuật toán sẽ chuyển\n",
      "\n",
      "sang câu hỏi tiếp theo: \"Động vật này có lông vũ không?\" Nếu đúng, động vật đó vẫn là\n",
      "\n",
      "Chim (ví dụ như chim cánh cụt, không bay nhưng có lông vũ). Nếu không, động vật đó\n",
      "\n",
      "sẽ được phân loại là Thú.\n",
      "\n",
      "Thuật toán cây quyết định này sử dụng một chuỗi các câu hỏi phân cấp để phân\n",
      "\n",
      "loại đối tượng dựa trên các đặc điểm rõ ràng và cụ thể. Điều này cho thấy cây quyết định\n",
      "\n",
      "là một công cụ trực quan và hiệu quả trong việc phân loại các đối tượng trong nhiều bài\n",
      "\n",
      "toán học máy và dữ liệu thực tế, nhờ vào việc chia nhỏ dữ liệu thành các quyết định dựa\n",
      "\n",
      "trên các thuộc tính riêng lẻ.\n",
      "\n",
      "2.1.2. Thuật toán cây quyết định phân lớp (Decision Tree Classification\n",
      "\n",
      "Algorithm):\n",
      "\n",
      "Thuật toán cây quyết định là một kỹ thuật học có giám sát, được sử dụng rộng rãi\n",
      "\n",
      "trong các bài toán phân loại và hồi quy. Nó thuộc nhóm các mô hình dự đoán có khả\n",
      "\n",
      "năng học hỏi từ dữ liệu đã có nhãn để đưa ra dự đoán cho các dữ liệu mới. Thuật toán\n",
      "\n",
      "cây quyết định đặc biệt hữu ích khi cần giải thích kết quả vì nó chia nhỏ các vấn đề phức\n",
      "\n",
      "tạp thành các quyết định nhỏ hơn và rõ ràng. Điều này giúp cho cây quyết định trở thành\n",
      "        You can also analyse data from CSV/Excel files reports, database.\n",
      "        Use the following rules:\n",
      "        - Always utilize all relevant context to generate a complete and accurate response.\n",
      "        - If the context lacks information, clearly state: \"Answer not found in the provided context.\"\n",
      "        - Respond in the same language as the input question.\n",
      "        - Provide structured responses (e.g., lists or bullet points) when applicable for clarity.\n",
      "        - Prioritize the most relevant information and avoid irrelevant details.\n",
      "        Context:\n",
      "        Chúng em xin chân thành cảm ơn!\n",
      "\n",
      "3\n",
      "\n",
      "CHƯƠNG 1: GIỚI THIỆU ĐỀ TÀI\n",
      "\n",
      "1.1. Tính cấp thiết của đề tài\n",
      "\n",
      "Chất lượng của rượu vang đóng vai trò quan trọng đối với cả người tiêu dùng lẫn ngành công nghiệp rượu. Phương pháp truyền thống để đánh giá chất lượng rượu thông qua các chuyên gia thường tốn nhiều thời gian và công sức. Hiện nay, các mô hình học máy đã trở thành công cụ quan trọng, thay thế một phần công việc của con người. Tuy nhiên, không phải tất cả các đặc điểm của rượu đều liên quan đến việc dự đoán chất lượng tốt hơn, mà chỉ có một số tính năng chính là cần thiết.\n",
      "\n",
      "Khi đánh giá chất lượng của một chai rượu, mỗi người thường có quan điểm khác nhau. Đối với nhiều nhà phê bình rượu, chất lượng thường liên quan đến việc phân biệt rượu \"ngon\" hay \"dở\", dựa trên sở thích cá nhân và sự hài lòng hoặc không hài lòng về rượu. Đánh giá này thường được đặt trong bối cảnh các tiêu chuẩn đã được thiết lập cho từng loại rượu, do vậy chất lượng không chỉ mang tính chủ quan mà còn phụ thuộc vào cả yếu tố cảm quan bên trong (mùi vị) và yếu tố bên ngoài (ngữ cảnh).\n",
      "\n",
      "Tuy nhiên, con người chúng ta chủ yếu bị ảnh hưởng bởi thị giác, do đó, hình dáng chai rượu thường ảnh hưởng đến cách chúng ta cảm nhận hương thơm và vị giác của rượu. Vì thế, để tránh thiên vị, những người thực sự muốn đánh giá rượu một cách khách quan thường nếm thử rượu mà không biết về nguồn gốc của nó, sử dụng các ly nếm rượu tiêu chuẩn hóa màu đen (ISO), chỉ đánh giá chất lượng dựa trên những gì rượu truyền tải đến khứu giác và vị giác.\n",
      "\n",
      "Vậy làm thế nào để đánh giá được một lượng lớn rượu về chất lượng tốt hay xấu? Bằng cách áp dụng học máy, chúng ta có thể sử dụng phương pháp sàng lọc, hay còn gọi là kỹ thuật phân lớp, để so sánh các mẫu rượu chất lượng cao với những mẫu khác, từ đó tìm ra các mẫu có chất lượng tương đồng nhất.\n",
      "\n",
      "1.2. Mục tiêu của đề tài\n",
      "\n",
      "Chúng em xin chân thành cảm ơn!\n",
      "\n",
      "3\n",
      "\n",
      "CHƯƠNG 1: GIỚI THIỆU ĐỀ TÀI\n",
      "\n",
      "1.1. Tính cấp thiết của đề tài\n",
      "\n",
      "Chất lượng của rượu vang đóng vai trò quan trọng đối với cả người tiêu dùng lẫn ngành công nghiệp rượu. Phương pháp truyền thống để đánh giá chất lượng rượu thông qua các chuyên gia thường tốn nhiều thời gian và công sức. Hiện nay, các mô hình học máy đã trở thành công cụ quan trọng, thay thế một phần công việc của con người. Tuy nhiên, không phải tất cả các đặc điểm của rượu đều liên quan đến việc dự đoán chất lượng tốt hơn, mà chỉ có một số tính năng chính là cần thiết.\n",
      "\n",
      "Khi đánh giá chất lượng của một chai rượu, mỗi người thường có quan điểm khác nhau. Đối với nhiều nhà phê bình rượu, chất lượng thường liên quan đến việc phân biệt rượu \"ngon\" hay \"dở\", dựa trên sở thích cá nhân và sự hài lòng hoặc không hài lòng về rượu. Đánh giá này thường được đặt trong bối cảnh các tiêu chuẩn đã được thiết lập cho từng loại rượu, do vậy chất lượng không chỉ mang tính chủ quan mà còn phụ thuộc vào cả yếu tố cảm quan bên trong (mùi vị) và yếu tố bên ngoài (ngữ cảnh).\n",
      "\n",
      "Tuy nhiên, con người chúng ta chủ yếu bị ảnh hưởng bởi thị giác, do đó, hình dáng chai rượu thường ảnh hưởng đến cách chúng ta cảm nhận hương thơm và vị giác của rượu. Vì thế, để tránh thiên vị, những người thực sự muốn đánh giá rượu một cách khách quan thường nếm thử rượu mà không biết về nguồn gốc của nó, sử dụng các ly nếm rượu tiêu chuẩn hóa màu đen (ISO), chỉ đánh giá chất lượng dựa trên những gì rượu truyền tải đến khứu giác và vị giác.\n",
      "\n",
      "Vậy làm thế nào để đánh giá được một lượng lớn rượu về chất lượng tốt hay xấu? Bằng cách áp dụng học máy, chúng ta có thể sử dụng phương pháp sàng lọc, hay còn gọi là kỹ thuật phân lớp, để so sánh các mẫu rượu chất lượng cao với những mẫu khác, từ đó tìm ra các mẫu có chất lượng tương đồng nhất.\n",
      "\n",
      "1.2. Mục tiêu của đề tài\n",
      "\n",
      "Chúng em xin chân thành cảm ơn!\n",
      "\n",
      "3\n",
      "\n",
      "CHƯƠNG 1: GIỚI THIỆU ĐỀ TÀI\n",
      "\n",
      "1.1. Tính cấp thiết của đề tài\n",
      "\n",
      "Chất lượng của rượu vang đóng vai trò quan trọng đối với cả người tiêu dùng lẫn ngành công nghiệp rượu. Phương pháp truyền thống để đánh giá chất lượng rượu thông qua các chuyên gia thường tốn nhiều thời gian và công sức. Hiện nay, các mô hình học máy đã trở thành công cụ quan trọng, thay thế một phần công việc của con người. Tuy nhiên, không phải tất cả các đặc điểm của rượu đều liên quan đến việc dự đoán chất lượng tốt hơn, mà chỉ có một số tính năng chính là cần thiết.\n",
      "\n",
      "Khi đánh giá chất lượng của một chai rượu, mỗi người thường có quan điểm khác nhau. Đối với nhiều nhà phê bình rượu, chất lượng thường liên quan đến việc phân biệt rượu \"ngon\" hay \"dở\", dựa trên sở thích cá nhân và sự hài lòng hoặc không hài lòng về rượu. Đánh giá này thường được đặt trong bối cảnh các tiêu chuẩn đã được thiết lập cho từng loại rượu, do vậy chất lượng không chỉ mang tính chủ quan mà còn phụ thuộc vào cả yếu tố cảm quan bên trong (mùi vị) và yếu tố bên ngoài (ngữ cảnh).\n",
      "\n",
      "Tuy nhiên, con người chúng ta chủ yếu bị ảnh hưởng bởi thị giác, do đó, hình dáng chai rượu thường ảnh hưởng đến cách chúng ta cảm nhận hương thơm và vị giác của rượu. Vì thế, để tránh thiên vị, những người thực sự muốn đánh giá rượu một cách khách quan thường nếm thử rượu mà không biết về nguồn gốc của nó, sử dụng các ly nếm rượu tiêu chuẩn hóa màu đen (ISO), chỉ đánh giá chất lượng dựa trên những gì rượu truyền tải đến khứu giác và vị giác.\n",
      "\n",
      "Vậy làm thế nào để đánh giá được một lượng lớn rượu về chất lượng tốt hay xấu? Bằng cách áp dụng học máy, chúng ta có thể sử dụng phương pháp sàng lọc, hay còn gọi là kỹ thuật phân lớp, để so sánh các mẫu rượu chất lượng cao với những mẫu khác, từ đó tìm ra các mẫu có chất lượng tương đồng nhất.\n",
      "\n",
      "1.2. Mục tiêu của đề tài\n",
      "\n",
      "một mô hình có thể dự đoán chính xác thành viên nào trong nhóm sẽ phản hồi một yêu\n",
      "\n",
      "cầu mà không cần biết chi tiết cách thức hoạt động của mô hình. Trong một số trường\n",
      "\n",
      "hợp khác, cây quyết định có thể giải thích lý do của một quyết định, điều này rất quan\n",
      "\n",
      "trọng trong các ứng dụng yêu cầu sự phân loại hay dự đoán chính xác.\n",
      "\n",
      "2.1.1. Ví dụ về cây quyết định:\n",
      "\n",
      "Thuật toán cây quyết định trong hình ảnh cung cấp một ví dụ rõ ràng về cách\n",
      "\n",
      "phân loại một loài động vật thành hai nhóm: \"Chim\" hoặc \"Thú\", dựa trên một loạt câu\n",
      "\n",
      "hỏi về đặc điểm sinh học.\n",
      "\n",
      "6\n",
      "\n",
      "Câu hỏi đầu tiên tại nút gốc là \"Động vật có thể bay không?\" Đây là điểm phân\n",
      "\n",
      "nhánh đầu tiên. Nếu câu trả lời là \"có\", thuật toán tiếp tục hỏi \"Động vật này có đẻ trứng\n",
      "\n",
      "không?\" Nếu đúng, động vật đó được phân loại là Chim. Nếu không, nó sẽ được phân\n",
      "\n",
      "loại là Thú (ví dụ như dơi). Ngược lại, nếu động vật không thể bay, thuật toán sẽ chuyển\n",
      "\n",
      "sang câu hỏi tiếp theo: \"Động vật này có lông vũ không?\" Nếu đúng, động vật đó vẫn là\n",
      "\n",
      "Chim (ví dụ như chim cánh cụt, không bay nhưng có lông vũ). Nếu không, động vật đó\n",
      "\n",
      "sẽ được phân loại là Thú.\n",
      "\n",
      "Thuật toán cây quyết định này sử dụng một chuỗi các câu hỏi phân cấp để phân\n",
      "\n",
      "loại đối tượng dựa trên các đặc điểm rõ ràng và cụ thể. Điều này cho thấy cây quyết định\n",
      "\n",
      "là một công cụ trực quan và hiệu quả trong việc phân loại các đối tượng trong nhiều bài\n",
      "\n",
      "toán học máy và dữ liệu thực tế, nhờ vào việc chia nhỏ dữ liệu thành các quyết định dựa\n",
      "\n",
      "trên các thuộc tính riêng lẻ.\n",
      "\n",
      "2.1.2. Thuật toán cây quyết định phân lớp (Decision Tree Classification\n",
      "\n",
      "Algorithm):\n",
      "\n",
      "Thuật toán cây quyết định là một kỹ thuật học có giám sát, được sử dụng rộng rãi\n",
      "\n",
      "trong các bài toán phân loại và hồi quy. Nó thuộc nhóm các mô hình dự đoán có khả\n",
      "\n",
      "năng học hỏi từ dữ liệu đã có nhãn để đưa ra dự đoán cho các dữ liệu mới. Thuật toán\n",
      "\n",
      "cây quyết định đặc biệt hữu ích khi cần giải thích kết quả vì nó chia nhỏ các vấn đề phức\n",
      "\n",
      "tạp thành các quyết định nhỏ hơn và rõ ràng. Điều này giúp cho cây quyết định trở thành\n",
      "\n",
      "một mô hình có thể dự đoán chính xác thành viên nào trong nhóm sẽ phản hồi một yêu\n",
      "\n",
      "cầu mà không cần biết chi tiết cách thức hoạt động của mô hình. Trong một số trường\n",
      "\n",
      "hợp khác, cây quyết định có thể giải thích lý do của một quyết định, điều này rất quan\n",
      "\n",
      "trọng trong các ứng dụng yêu cầu sự phân loại hay dự đoán chính xác.\n",
      "\n",
      "2.1.1. Ví dụ về cây quyết định:\n",
      "\n",
      "Thuật toán cây quyết định trong hình ảnh cung cấp một ví dụ rõ ràng về cách\n",
      "\n",
      "phân loại một loài động vật thành hai nhóm: \"Chim\" hoặc \"Thú\", dựa trên một loạt câu\n",
      "\n",
      "hỏi về đặc điểm sinh học.\n",
      "\n",
      "6\n",
      "\n",
      "Câu hỏi đầu tiên tại nút gốc là \"Động vật có thể bay không?\" Đây là điểm phân\n",
      "\n",
      "nhánh đầu tiên. Nếu câu trả lời là \"có\", thuật toán tiếp tục hỏi \"Động vật này có đẻ trứng\n",
      "\n",
      "không?\" Nếu đúng, động vật đó được phân loại là Chim. Nếu không, nó sẽ được phân\n",
      "\n",
      "loại là Thú (ví dụ như dơi). Ngược lại, nếu động vật không thể bay, thuật toán sẽ chuyển\n",
      "\n",
      "sang câu hỏi tiếp theo: \"Động vật này có lông vũ không?\" Nếu đúng, động vật đó vẫn là\n",
      "\n",
      "Chim (ví dụ như chim cánh cụt, không bay nhưng có lông vũ). Nếu không, động vật đó\n",
      "\n",
      "sẽ được phân loại là Thú.\n",
      "\n",
      "Thuật toán cây quyết định này sử dụng một chuỗi các câu hỏi phân cấp để phân\n",
      "\n",
      "loại đối tượng dựa trên các đặc điểm rõ ràng và cụ thể. Điều này cho thấy cây quyết định\n",
      "\n",
      "là một công cụ trực quan và hiệu quả trong việc phân loại các đối tượng trong nhiều bài\n",
      "\n",
      "toán học máy và dữ liệu thực tế, nhờ vào việc chia nhỏ dữ liệu thành các quyết định dựa\n",
      "\n",
      "trên các thuộc tính riêng lẻ.\n",
      "\n",
      "2.1.2. Thuật toán cây quyết định phân lớp (Decision Tree Classification\n",
      "\n",
      "Algorithm):\n",
      "\n",
      "Thuật toán cây quyết định là một kỹ thuật học có giám sát, được sử dụng rộng rãi\n",
      "\n",
      "trong các bài toán phân loại và hồi quy. Nó thuộc nhóm các mô hình dự đoán có khả\n",
      "\n",
      "năng học hỏi từ dữ liệu đã có nhãn để đưa ra dự đoán cho các dữ liệu mới. Thuật toán\n",
      "\n",
      "cây quyết định đặc biệt hữu ích khi cần giải thích kết quả vì nó chia nhỏ các vấn đề phức\n",
      "\n",
      "tạp thành các quyết định nhỏ hơn và rõ ràng. Điều này giúp cho cây quyết định trở thành\n",
      "        Question:\n",
      "        Cam onon\n",
      "        Response:.\n",
      "        \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Cam onon', 'result': 'Chúng em xin chân thành cảm ơn!\\n'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_with_pdf(\"Cam onon\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
